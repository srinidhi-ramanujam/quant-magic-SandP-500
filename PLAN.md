# Development Plan - S&P 500 Financial Analysis Platform

**Product**: AI-powered financial analytics via natural language  
**Approach**: Incremental delivery with clear exit criteria  
**Current Phase**: Phase 1 Complete â†’ Phase 2 Starting  
**Test Status**: 118 passed / 2 skipped / 2 xfailed

---

## Executive Summary

### What's Working âœ…
- **Phase 0**: PoC with 10/10 questions correct
- **Phase 1**: LLM-first entity extraction & template selection with deterministic fallback when Azure is unavailable
- **Data**: 15.5M+ facts, 589 companies, 27 SQL templates (schema-aligned)
- **Tests**: 118 passing (integration marked/skipped), full template execution harness, Phase 0 PoC suite fast again
- **CLI**: Interactive REPL + single-shot modes now default to LLM paths when credentials exist

### What's Next ðŸš€
- **Phase 2**: Custom SQL generation + validation + coverage expansion
- **Target**: 86+/171 simple questions passing (50%+)
- **Timeline**: ~20-25 hours of focused development

### Long-Term Goal
90% pass rate on all 410 evaluation questions (369+/410)

---

## âœ… Phase 0: Foundation (COMPLETE)

**Goal**: Prove end-to-end flow with minimal templates

### Deliverables
- âœ… Data layer: DuckDB + parquet (15.5M+ facts)
- âœ… Entity extraction: Deterministic patterns
- âœ… SQL generation: 3 templates (sector_count, company_cik, company_sector)
- âœ… Response formatting: Natural language output
- âœ… CLI: Interactive REPL + single-shot modes
- âœ… Tests: 55 passing (100%)
- âœ… Telemetry: Logging, timing, request tracking

### Exit Criteria
- âœ… 10/10 PoC questions correct
- âœ… <2s response time
- âœ… All tests passing
- âœ… Clean architecture with Pydantic models

**Completed**: November 3, 2025

---

## âœ… Phase 1: AI Integration (COMPLETE)

**Goal**: Add Azure OpenAI for entity extraction and template selection

### Deliverables

#### 1. Azure OpenAI Integration âœ…
- âœ… `src/azure_client.py`: Production-ready wrapper (720 lines)
- âœ… GPT-5 deployment with retry logic + circuit breaker
- âœ… Token tracking with reasoning breakdown
- âœ… 7 new Pydantic models for LLM interactions
- âœ… 20 tests passing (100%)

#### 2. LLM Entity Extraction âœ…
- âœ… `src/prompts.py`: 400-line prompt with 10 few-shot examples
- âœ… `LLMEntityRequest` and `LLMEntityResponse` models
- âœ… LLM-first approach with deterministic fallback
- âœ… Robust JSON parsing (handles markdown code blocks)
- âœ… 13 tests passing (11 unit + 2 integration)
- âœ… CLI command: `--test-entity-extraction --use-llm`

#### 3. Hybrid Template Selection âœ…
- âœ… 3-tier routing logic in `src/sql_generator.py`:
  - Fast path (â‰¥0.8 confidence): Skip LLM, use template directly
  - LLM confirmation (0.5-0.8): LLM validates template choice
  - LLM fallback (<0.5): LLM selects from all templates
- âœ… `_select_template_with_llm()` with retry and telemetry
- âœ… Template expansion: 3 â†’ 27 patterns
- âœ… 16 tests passing (11 unit + 2 integration + 3 edge cases)

#### 4. Template Expansion âœ…
- âœ… 27 SQL templates across 7 categories:
  - Company lookups (4): CIK, headquarters, incorporation, sector
  - Sector analysis (5): counts, lists, thresholds
  - Geographic (3): HQ state, incorporation state
  - Financial metrics (5): revenue, assets, equity, net income
  - Ratios (3): ROE, current ratio, debt-to-equity
  - Revenue time series (3): YoY, multi-year, quarterly
  - Profitability time series (2): net margin, operating margin
- âœ… Flexible regex patterns for natural language matching
- âœ… All templates validated against real questions
- âœ… Template SQL aligned with DuckDB schema and covered by automated execution tests

#### 5. Operational Readiness âœ…
- âœ… LLM entity extraction and template selection default to Azure GPT-5 when credentials are present
- âœ… Safe fallback to deterministic paths when Azure access is unavailable

### Exit Criteria
- âœ… Azure OpenAI integrated with GPT-5
- âœ… LLM entity extraction: 13/13 tests passing
- âœ… Hybrid template selection working end-to-end
- âœ… 27 templates operational
- âœ… 100/104 tests passing (96.2%)
- âœ… No regressions from Phase 0

**Completed**: November 4, 2025

---

## ðŸš€ Phase 2: SQL Generation & Coverage (NEXT)

**Goal**: Generate custom SQL for non-template questions and achieve 50%+ coverage

**Current Coverage**: 47/279 evaluation questions matched (16.8%)  
**Target Coverage**: 86+/171 simple questions (50%+)

### Phase 2A: Custom SQL Generation (8-10 hours)

Current status: schema docs + wiring landed, prompt/test scaffolding pending.

#### Deliverables
- [x] **Schema Documentation** (`src/schema_docs.py`)
  - Table/column catalog with join hints
  - Metric taxonomy for top 50 tags
  - Helper accessors surfaced to prompts
- [x] **Custom SQL Generator stub** (`SQLGenerator._generate_custom_sql`)
  - Uses schema docs + LLM fallback when templates miss
  - Deterministic guardrails around empty/invalid SQL
- [x] **Prompt Refinement** (`get_sql_custom_generation_prompt`)
  - Added curated few-shot set covering joins, aggregations, ranking
  - Embedded failure-mode guidance and domain hints (currency, thresholds, ordering)
  - Centralized prompt wiring through Azure client
- [x] **Validation Hooks**
  - Enforce read-only (`SELECT`) queries with multi-statement rejection
  - Require `NUM` joins to pass through `SUB`, block unknown tables
  - Emit telemetry for every custom SQL attempt with pass/fail reason
- [x] **Test Suite** (`tests/test_sql_generator.py`)
  - Unit coverage for prompt assembly, guardrails, telemetry logging
  - Mocked LLM success/failure paths for custom SQL generation
  - Integration smoke behind `ENABLE_AZURE_INTEGRATION_TESTS` (pending credentials toggle)

#### Exit Criteria
- [ ] 10+ non-template evaluation questions answered via custom SQL
- [ ] Prompt/test coverage prevents regressions on schema drift
- [ ] Telemetry shows success rate & latency for custom SQL path
- [ ] No drop in existing deterministic/template coverage

---

### Phase 2B: Two-Pass Validation (6-8 hours)

#### Deliverables
- [x] **SQL Validator** (`src/sql_validator.py`)
  - Pass 1: Static scan for DDL/DML + unbounded deletes/updates
  - Pass 2: LLM intent alignment with structured verdict (`valid`, `reason`, `confidence`)
  - Configurable retry budget surfaced via config
  - Emits telemetry (`sql_validation` + `llm_calls` entries for every attempt)
- [x] **Validation Prompts** (`get_sql_semantic_validation_prompt`)
  - Include schema snippets + natural-language question
  - Require JSON verdict with normalized rationale bullets
  - Provide examples for allow/deny cases via curated instructions
- [x] **Pipeline Integration**
  - Invoke validator for all custom SQL paths (templates remain configurable next)
  - Short-circuit response if validation fails; return actionable error
  - Persist validation metadata on the request context for downstream logging
- [x] **Tests** (`tests/test_sql_validator.py`, `tests/test_sql_generator.py`)
  - Static checks (dangerous SQL, CTE handling)
  - Mocked LLM semantic checks (match/mismatch)
  - Telemetry assertions for success/failure paths

#### Exit Criteria
- [ ] 100% of generated SQL passes static scan before execution
- [ ] Semantic validation confidence â‰¥0.8 on green paths
- [ ] Workbook entries capture validation outcome & confidence
- [ ] No increase in execution failures during eval sweeps

---

### Phase 2C: Coverage Expansion (6-8 hours)

#### Deliverables
- [x] **Evaluation Runner Enhancements** (`scripts/run_eval_suite.py`)
  - CLI flags for tiers/custom questions
  - Telemetry logging into `evaluation/EVAL_WORKBOOK.csv`
  - Auto-scoring (5/3/1) with tolerance awareness
- [ ] **Workbook Analytics**
  - Add summarizer script for pass-rate by tag/category
  - Reverse chronological ordering (done) + filters for regressions
  - Derive top failure taxonomies (template miss, entity miss, SQL error)
- [ ] **Iteration 1 â€“ High-Impact Fixes**
  - Close remaining simple-tier numeric deltas (gross margin, other income)
  - Add EBITDA / acquisition threshold templates and align expected answers
  - Target â‰¥150/171 simple-tier scores at 5
- [ ] **Iteration 2 â€“ Medium Tier Prep**
  - Refresh medium/time-series expected answers to match current parquet data
  - Ensure entity extractor & template loader cover fiscal period questions
  - Stand up regression templates for multi-year KPIs (inc. top-10 by sector)
- [ ] **Reporting**
  - Nightly baseline run stored as `RUN_latest`
  - Markdown summary snippet for PRs (pass %, top failures, new fixes)

#### Exit Criteria
- [ ] Simple tier â‰¥50% coverage with quality=5
- [ ] Medium tier pilot (>30% quality=5) ready for next iteration
- [ ] Regression alarms when template answers drift from parquet data
- [ ] Documented playbook for updating expected answers safely
  - Target: +15-20 questions passing

- [ ] **Iteration 3**: Reach 50% threshold
  - Fine-tune prompts
  - Add edge case handling
  - Target: 86+ total passing (50%+)

- [ ] **Documentation**: Update README.md and PLAN.md
  - Document what works and what doesn't
  - List known limitations
  - Update performance metrics

#### Exit Criteria
- [ ] 86+/171 simple questions passing (50%+)
- [ ] All Phase 2 tests passing (30+ new tests)
- [ ] Evaluation report generated
- [ ] Documentation updated
- [ ] Clean codebase (formatted, linted)

---

### Time-Series Template Roadmap (NEW)

**Goal**: Land reusable parameterized SQL templates that unlock the curated 25-question time-series suite without ad-hoc LLM work.

| Template Family | Template IDs | Scope / Notes |
|-----------------|--------------|---------------|
| Profitability trends | `profit_margin_consistency_trend`, `consumer_staples_gross_margin_trend`, `hardware_gross_margin_trend`, `cross_sector_gross_margin_spread`, `ebitda_margin_improvement_rank`, `roe_trend_named_semis`, `bank_roe_consecutive_threshold`, `roe_vs_revenue_growth_flag` | Multi-year margin/ROE deltas leveraging revenue, cost, and equity tags plus cohort filters. |
| Cash flow & capital allocation | `sector_free_cash_flow_trend`, `top_tech_cfo_trend`, `cfo_to_net_income_ratio_trend`, `healthcare_cfo_to_capex_ratio_trend`, `operating_cf_volatility_sector` | Operating cash flow contrasted with capex or net income to gauge quality of earnings and reinvestment. |
| Balance sheet health | `current_ratio_trend`, `cash_to_assets_ratio_trend`, `equity_to_assets_ratio_trend`, `working_capital_cash_cycle_trend`, `retail_cash_conversion_cycle_trend` | Liquidity metrics derived from assets/liabilities plus CCC decomposition (DSO/DIO/DPO). |
| Leverage & debt motion | `debt_reduction_progression`, `net_debt_to_ebitda_trend`, `energy_roe_threshold_detector` | Tracks leverage changes, approximated EBITDA, and high-ROE streaks for capital-intensive sectors. |
| Efficiency metrics | `operating_margin_trend`, `inventory_turnover_trend`, `asset_turnover_trend` | Ratio trends based on inventory, revenue, and asset balances; reusable across cohorts. |

**Next Actions**
1. Implement slot extractors + SQL builders for the profitability and cash-flow families (highest coverage impact).
2. Register each template in `data/template_intents.json` with clear natural-language exemplars for FAISS retrieval.
3. Add focused unit tests per family that validate metric math against DuckDB snapshots.
4. Re-run `scripts/run_eval_suite.py --suite time_series` and append telemetry to `evaluation/EVAL_WORKBOOK.csv`.

---

### Phase 2D: Hybrid Retrieval Initiative (Next)

**Goal**: Replace LLM-heavy entity extraction and template selection with hybrid (keyword + embedding) retrieval while demonstrating measurable gains.

#### Metrics to Track (Baseline vs. Hybrid)
- **Entity Extraction Accuracy** â€“ % of evaluation questions where all required slots match the expected context (derived from workbook + entity diff script). Target â‰¥98%.
- **Template Hit Rate** â€“ % of questions routed to the correct template without fallback. Target â‰¥95%.
- **LLM Reliance** â€“ Average number of LLM calls per question (entity + template + custom SQL). Target near-zero for template-backed queries.
- **Latency** â€“ Median end-to-end time per question (captured in workbook metadata). Target â‰¥30% improvement vs RUN_020 baseline.
- **Custom SQL Success** â€“ Semantic validator pass rate and latency for the remaining LLM-generated SQL cases to ensure no regressions.

#### Deliverables
- [ ] **Entity Catalog + Embeddings** â€“ Curated dictionaries (sectors, jurisdictions, metrics, time phrases, question types) embedded via `sentence-transformers` and stored in FAISS/Chroma.
- [ ] **HybridEntityRetriever** â€“ Combines existing regex/threshold hints with embedding similarity to produce canonical slot values with confidence.
- [ ] **TemplateIntentRetriever** â€“ Keyword-filter + embedding similarity over template intent cards to select the template without hitting the LLM.
- [ ] **Telemetry & Metrics Script** â€“ Log retrieval confidences/result and ship a notebook/script that compares baseline vs. hybrid runs.
- [ ] **Runbook Updates** â€“ README/PLAN instructions on refreshing embeddings, tuning thresholds, and interpreting the metrics dashboard.

#### Exit Criteria
- [ ] â‰¥95% of simple-tier questions resolved without LLM entity/template calls.
- [ ] Entity extraction accuracy â‰¥98% on the evaluation suite.
- [ ] Template hit rate â‰¥95% with no quality regression.
- [ ] Median latency improvement â‰¥30% vs. RUN_020.
- [ ] Metrics dashboard attached to PRs demonstrating the improvement.

---

## Phase 3: Advanced Features (FUTURE)

**Goal**: Support medium questions and time series analysis

### Deliverables (Planned)
- [ ] Medium question support (multi-step analysis)
  - Ratio calculations with context
  - Company comparisons
  - Sector benchmarking
  - Target: 35+/50 medium questions (70%+)

- [ ] Time series analysis
  - Trend detection
  - Growth rate calculations
  - Seasonality handling
  - Target: 32+/40 time series questions (80%+)

- [ ] Enhanced response formatting
  - Tables and charts (JSON spec for UI)
  - Comparisons with context
  - Sector benchmarks

- [ ] Caching layer
  - In-memory cache for repeat queries
  - 40%+ cache hit rate target

### Exit Criteria (Planned)
- [ ] 70%+ medium questions passing
- [ ] 80%+ time series questions passing
- [ ] Response formatting enhanced
- [ ] Cache working with metrics

---

## Phase 4: Production Ready (FUTURE)

**Goal**: 90%+ overall pass rate and production deployment

### Deliverables (Planned)
- [ ] Complex question support
  - Strategic analysis
  - Multi-dimensional queries
  - Target: 19+/25 complex questions (75%+)

- [ ] Web UI integration
  - Export Pydantic schemas to TypeScript
  - REST API wrapper around CLI
  - Mock FastAPI for UI testing

- [ ] Performance optimization
  - Query result caching
  - Template pre-compilation
  - Target: <1s for 80%+ of queries

- [ ] Production deployment
  - Azure deployment artifacts
  - Monitoring and alerting
  - Cost tracking

### Exit Criteria (Planned)
- [ ] 369+/410 questions passing (90%+)
- [ ] Production-ready API
- [ ] UI integration proven
- [ ] Deployment automated

---

## Success Metrics

### Coverage Targets by Phase
| Phase | Simple | Medium | Complex | Time Series | Overall |
|-------|--------|--------|---------|-------------|---------|
| Phase 0 | 10/295 (3%) | 0/50 | 0/25 | 0/40 | 10/410 (2%) |
| Phase 1 | ~47/171 (27%)* | 0/50 | 0/25 | 0/40 | ~47/410 (11%) |
| **Phase 2** | **86+/171 (50%+)** | 0/50 | 0/25 | 0/40 | **86+/410 (21%+)** |
| Phase 3 | 120+/171 (70%+) | 35+/50 (70%) | 0/25 | 32+/40 (80%) | 187+/410 (45%+) |
| Phase 4 | 150+/171 (87%+) | 43+/50 (85%+) | 19+/25 (75%+) | 32+/40 (80%+) | **369+/410 (90%+)** âœ… |

*Phase 1 template matching shows 47/279 matched, but actual execution not validated yet

### Performance Targets
- **Latency**: <1s for template path, <10s for LLM path
- **Accuracy**: 90%+ overall (369+/410 questions)
- **Test Coverage**: 100% pass rate on unit/integration tests
- **Code Quality**: <5,000 total lines, Black formatted, type-hinted

---

## Technical Architecture

### Hybrid AI System

```
User Question
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Entity Extraction    â”‚ â† LLM-enhanced (GPT-5)
â”‚    - Companies          â”‚
â”‚    - Metrics            â”‚
â”‚    - Time periods       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Template Selection   â”‚ â† Hybrid Router
â”‚    - Fast path (â‰¥0.8)   â”‚   â€¢ No LLM call
â”‚    - LLM confirm (0.5-0.8) â”‚   â€¢ LLM validates
â”‚    - LLM fallback (<0.5)â”‚   â€¢ LLM selects
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. SQL Generation       â”‚
â”‚    - Template-based     â”‚ â† 27 patterns
â”‚    - Custom (LLM)       â”‚ â† Phase 2
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Validation (2-pass)  â”‚ â† Phase 2
â”‚    - Syntax check       â”‚
â”‚    - Semantic check (LLM)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Query Execution      â”‚
â”‚    DuckDB on parquet    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Response Formatting  â”‚
â”‚    Natural language     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
        Answer
```

### Technology Stack

**Core**:
- Python 3.11+
- DuckDB (OLAP queries on parquet)
- Pydantic v2 (data validation)
- pandas + pyarrow (data manipulation)

**AI**:
- Azure OpenAI (GPT-5 deployment)
- Responses API with retry logic
- Token tracking + cost monitoring
- Circuit breaker pattern

**Testing**:
- pytest (104 tests)
- 410 evaluation questions
- Unit + integration + end-to-end tests

**Code Quality**:
- Black (formatting)
- Type hints (100% coverage)
- Pydantic models (all contracts)
- ~2,500 lines application code

---

## Development Workflow

### Daily Process
```bash
# 1. Activate environment
source .venv/bin/activate

# 2. Run tests
python -m pytest tests/ -v

# 3. Work on feature
# ... write test first ...
# ... implement feature ...

# 4. Run tests again
python -m pytest tests/test_your_feature.py -v

# 5. Format code
python -m black src/ tests/

# 6. Commit
git add .
git commit -m "feat: description"
```

### Phase 2 Workflow
1. **Schema Documentation** â†’ tests â†’ implement
2. **Custom SQL Generation** â†’ tests â†’ implement
3. **Two-Pass Validation** â†’ tests â†’ implement
4. **Evaluation** â†’ iterate â†’ achieve 50%+
5. **Document** â†’ commit â†’ ready for Phase 3

---

## Risk Mitigation

### Known Risks
1. **LLM Cost**: Mitigated by fast path (70%+ queries skip LLM)
2. **LLM Latency**: Acceptable for demo, can optimize later
3. **SQL Injection**: Validation blocks dangerous keywords
4. **Template Coverage**: Custom SQL generator fills gaps

### Quality Gates
- All tests must pass before proceeding to next phase
- No regressions tolerated (100+ tests must stay green)
- Exit criteria must be met before claiming phase complete

---

## Current File Structure

```
quant-magic-SandP-500/
â”œâ”€â”€ data/parquet/              # 253MB data
â”‚   â”œâ”€â”€ num.parquet            # 15.5M facts
â”‚   â”œâ”€â”€ companies_with_sectors.parquet  # 589 companies
â”‚   â”œâ”€â”€ query_intelligence.parquet      # 27 templates
â”‚   â”œâ”€â”€ financial_concepts.parquet
â”‚   â”œâ”€â”€ financial_ratios_definitions.parquet
â”‚   â””â”€â”€ company_aliases.csv    # 161 aliases
â”‚
â”œâ”€â”€ src/                       # ~2,500 lines
â”‚   â”œâ”€â”€ azure_client.py        # 720 lines - Azure OpenAI wrapper
â”‚   â”œâ”€â”€ entity_extractor.py    # LLM + deterministic extraction
â”‚   â”œâ”€â”€ sql_generator.py       # Hybrid template selection
â”‚   â”œâ”€â”€ prompts.py             # LLM prompt templates
â”‚   â”œâ”€â”€ models.py              # 14 Pydantic models
â”‚   â”œâ”€â”€ cli.py                 # Interactive CLI
â”‚   â”œâ”€â”€ query_engine.py        # DuckDB wrapper
â”‚   â”œâ”€â”€ response_formatter.py  # NL formatting
â”‚   â”œâ”€â”€ intelligence_loader.py # Template loader
â”‚   â”œâ”€â”€ telemetry.py           # Logging + timing
â”‚   â””â”€â”€ config.py              # Configuration
â”‚
â”œâ”€â”€ tests/                     # 104 tests (100 passing)
â”‚   â”œâ”€â”€ test_entity_extractor_llm.py    # 13 tests
â”‚   â”œâ”€â”€ test_sql_generator_hybrid.py    # 16 tests
â”‚   â”œâ”€â”€ test_eval_poc.py                # 11 tests (Phase 0)
â”‚   â”œâ”€â”€ test_azure_client.py            # 20 tests
â”‚   â””â”€â”€ ... (9 other test files)
â”‚
â”œâ”€â”€ evaluation/questions/      # 410 questions
â”‚   â”œâ”€â”€ simple_lineitem.json   # 171 validated (from 295)
â”‚   â”œâ”€â”€ medium_analysis.json   # 50 questions
â”‚   â”œâ”€â”€ complex_strategic.json # 25 questions
â”‚   â””â”€â”€ time_series_analysis.json # 40 questions
â”‚
â”œâ”€â”€ README.md                  # User documentation
â””â”€â”€ PLAN.md                    # This file
```

---

## Phase 2 Detailed Timeline

### Week 1: Core Implementation (14-18 hours)
- **Day 1-2**: Schema docs + custom SQL generation (8-10 hours)
- **Day 3-4**: Two-pass validation (6-8 hours)
- **Checkpoint**: 20+ new tests passing, no regressions

### Week 2: Coverage Expansion (6-8 hours)
- **Day 5**: Evaluation runner + Iteration 1 (3 hours)
- **Day 6**: Iteration 2 + Iteration 3 (3 hours)
- **Day 7**: Documentation + cleanup (2 hours)
- **Checkpoint**: 86+/171 passing (50%+), Phase 2 complete

**Total Estimate**: 20-26 hours of focused development

---

## Next Actions (Phase 2 Start)

1. âœ… **Complete Phase 1 cleanup** (DONE)
   - All tests passing (100/104)
   - Documentation updated
   - Code committed

2. [ ] **Phase 2A: Schema Documentation**
   - Create `src/schema_docs.py`
   - Document all tables, columns, join patterns
   - Add XBRL tag reference

3. [ ] **Phase 2A: Custom SQL Generation**
   - Implement `_generate_custom_sql()` in `sql_generator.py`
   - Create prompt in `prompts.py`
   - Write 10 tests

4. [ ] **Phase 2B: Validation**
   - Create `src/sql_validator.py`
   - Implement two-pass validation
   - Write 10 tests

5. [ ] **Phase 2C: Coverage Expansion**
- Audit simple-tier templates for schema compatibility (replace stprinc/companies_with_sectors usage, ensure DuckDB column names match)
- Add deterministic/custom templates for missing simple-tier patterns (currency usage, footnote counts, CIK lookups, latest filing dates, etc.)
- Refresh simple-tier evaluation answers/tolerances once SQL alignment is complete

   - Run evaluation (171 simple questions)
   - Fix issues iteratively
   - Achieve 86+ passing (50%+)

---

## Parallel UI & Codespaces Roadmap

These tracks run alongside Phase 2 backend work so the product can demo via a browser from GitHub Codespaces.

### 1. Codespaces & Devcontainer Enablement âœ…
- âœ… Add `.devcontainer/devcontainer.json` using `mcr.microsoft.com/devcontainers/python:3.11` with Node 20 feature.
- âœ… Preinstall Python dependencies (`pip install -r requirements.txt`) and prep for future `npm install`.
- âœ… Forward ports 8000 (FastAPI) and 5173 (Vite), enable public URLs, and document DuckDB parquet handling plus required Azure secrets via Codespaces settings.

### 2. FastAPI Service Layer
- Introduce `src/services/query_service.py` (wraps entity extraction, SQL generation, execution, telemetry).
- Stand up `src/api/app.py` exposing `POST /query` returning structured answers, SQL, and metadata; include graceful fallbacks when Azure creds are absent.
- Add pytest coverage using `TestClient` for happy path, validation errors, and failure handling.

### 3. Frontend Scaffold (React + HTMX) âœ…
- âœ… Create `frontend/` via Vite (React + TypeScript); configure Tailwind/PostCSS and load HTMX for progressive enhancement.
- âœ… Implement initial query form + results shell calling FastAPI; keep composition extensible for future charts/visuals.
- âœ… Skip authentication for first release; rely on Codespaces share links while leaving hooks for future auth layers.
- âœ… Establish fetch client conventions and state management (start lightweight hooks, evaluate React Query later).

#### Chat Interface (Completed November 6, 2025)
- âœ… **Production-Ready Chat UI** - Modern chat interface with:
  - Left sidebar (fixed): ASCENDION branding, chat history, quick access menu, settings
  - Main chat area (scrollable): conversation thread with user questions and AI responses
  - Bottom text input with aligned send button and auto-resize
  - API connection status indicator (top right)
  - Color scheme: Professional indigo/blue palette (indigo-600 to blue-500 gradient for user messages)
  - Chat session management and history tracking
  - Built with React + TypeScript + Tailwind CSS + Vite
  - Responsive design with smooth scrolling and animations

### 4. Tooling & Documentation âœ…
- âœ… Provide shared commands (Makefile or tasks) for `pytest -m "not integration"`, `uvicorn src.api.app:app --reload`, and `npm run dev`.
- âœ… Update onboarding docs (`README.md`, AGENTS.md if needed) with Codespaces setup, env vars, and run instructions.
- âœ… Document chat interface features, setup, and usage patterns.
- âœ… Capture open questions (parquet distribution, streaming updates) for next iteration before implementation.

---

**Status**: Phase 1 Complete (100/104 tests passing)  
**Next**: Phase 2 - Custom SQL + Validation + Coverage  
**Goal**: 86+/171 simple questions (50%+)  
**Timeline**: 20-26 hours

---

**Last Updated**: November 6, 2025  
**Version**: 2.1 (Added Chat Interface)
